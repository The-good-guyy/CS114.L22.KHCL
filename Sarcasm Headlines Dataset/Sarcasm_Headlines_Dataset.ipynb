{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sarcasm_Headlines_Dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO5phI8y1jGY8OavJjo3TgS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noeffortnomoney/CS114.L22.KHCL/blob/main/Sarcasm%20Headlines%20Dataset/Sarcasm_Headlines_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHBSXI7MZ8L-"
      },
      "source": [
        "#CS114.L22.KHCL\n",
        "- Nhóm:\n",
        "\n",
        "  + Trương Quốc Bình - 19521270\n",
        "\n",
        "  + Trần Vĩ Hào - 19521482\n",
        "\n",
        "  + Tô Thanh Hiền - 19521490"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIpTKkAC4pMh"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "from google.colab import files\n",
        "url='https://www.betootaadvocate.com/category/sports/'\n",
        "page=requests.get(url).text\n",
        "soup=BeautifulSoup(page,'html.parser')\n",
        "articles=soup.find_all('div',{'class':\"td_module_12 td_module_wrap td-animation-stack\" })\n",
        "titles=[]\n",
        "for article in articles:\n",
        "  title=article.find('a').get_text()\n",
        "  link=article.find('a')['href']\n",
        "  titles.append([title,link])\n",
        "titles\n",
        "time_wait=6\n",
        "urls='https://www.betootaadvocate.com/category/sports/page/{}/'\n",
        "for page in range(2,107):\n",
        "  print(page)\n",
        "  requestz=requests.get(urls.format(page)).text\n",
        "  soup=BeautifulSoup(requestz,'html.parser')\n",
        "  articles=soup.find_all('div',{'class':\"td_module_12 td_module_wrap td-animation-stack\" })\n",
        "  for article in articles:\n",
        "    title=article.find('a').get_text()\n",
        "    link=article.find('a')['href']\n",
        "    titles.append([title,link])\n",
        "    time_have_wait=time_wait+random.randint(122,255)/random.randint(50,173) - random.randint(0,2)*random.randint(0,2)\n",
        "  print('Time have to wait is %d',time_have_wait)\n",
        "  time.sleep(time_have_wait) \n",
        "print(titles)\n",
        "for article in articles:\n",
        "  print(article)\n",
        "df=pd.DataFrame(titles)\n",
        "df.to_csv('betootaadvocatesport.csv',encoding='utf-8-sig')\n",
        "files.download('betootaadvocatesport.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFvywB4cWd0O"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "from google.colab import files\n",
        "url='https://www.betootaadvocate.com/category/entertainment/'\n",
        "page=requests.get(url).text\n",
        "soup=BeautifulSoup(page,'html.parser')\n",
        "articles=soup.find_all('div',{'class':\"td_module_12 td_module_wrap td-animation-stack\" })\n",
        "titles=[]\n",
        "for article in articles:\n",
        "  title=article.find('a').get_text()\n",
        "  link=article.find('a')['href']\n",
        "  titles.append([title,link])\n",
        "time_wait=6\n",
        "urls='https://www.betootaadvocate.com/category/entertainment/page/{}/'\n",
        "for page in range(2,106):\n",
        "  print(page)\n",
        "  requestz=requests.get(urls.format(page)).text\n",
        "  soup=BeautifulSoup(requestz,'html.parser')\n",
        "  articles=soup.find_all('div',{'class':\"td_module_12 td_module_wrap td-animation-stack\" })\n",
        "  for article in articles:\n",
        "    title=article.find('a').get_text()\n",
        "    titles.append(title)\n",
        "    time_have_wait=time_wait+random.randint(122,255)/random.randint(50,173) - random.randint(0,2)*random.randint(0,2)\n",
        "  print('Time have to wait is %d',time_have_wait)\n",
        "  time.sleep(time_have_wait) \n",
        "df=pd.DataFrame(titles)\n",
        "df.to_csv('betootaadvocatepolitic.csv',encoding='utf-8-sig')\n",
        "files.download('betootaadvocatesport.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0p1fndiV290"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "url='https://www.thebeaverton.com/'\n",
        "page=requests.get(url).text\n",
        "soup=BeautifulSoup(page,'html.parser')\n",
        "articles=soup.find_all('article',{'role':\"article\"})\n",
        "titles=[]\n",
        "for article in articles:\n",
        "  title=article.find('h3').get_text()\n",
        "  link=article.find('h3').find('a')['href']\n",
        "  titles.append([title,link])\n",
        "titles\n",
        "time_wait=6\n",
        "urls='https://www.thebeaverton.com/page/{}/'\n",
        "for page in range(2,779):\n",
        "  print(page)\n",
        "  requestz=requests.get(urls.format(page)).text\n",
        "  soup=BeautifulSoup(requestz,'html.parser')\n",
        "  articles=soup.find_all('article',{'role':\"article\"})\n",
        "  for article in articles:\n",
        "    title=article.find('h3').get_text()\n",
        "    link=article.find('h3').find('a')['href']\n",
        "    titles.append([title,link])\n",
        "    time_have_wait=time_wait+random.randint(122,255)/random.randint(30,173)- random.randint(0,2)*random.randint(0,15)/random.randint(8,15)\n",
        "  print('Time have to wait is %d',time_have_wait)\n",
        "  time.sleep(time_have_wait)\n",
        "df.to_csv('thebeaverton.csv',encoding='utf-8-sig')\n",
        "df=pd.DataFrame(titles)\n",
        "from google.colab import files\n",
        "files.download('thebeaverton.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YNRC9BAXJsU"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "from google.colab import files\n",
        "url='https://www.newyorker.com/humor/borowitz-report'\n",
        "page=requests.get(url).text\n",
        "soup=BeautifulSoup(page,'html.parser')\n",
        "articles=soup.find_all('li',{'class':\"River__riverItem___3huWr\" })\n",
        "titles=[]\n",
        "for article in articles:\n",
        "  title=article.find('h4').get_text()\n",
        "  link='https://www.newyorker.com'+article.find('a')['href']\n",
        "  titles.append([title,link])\n",
        "titles\n",
        "time_wait=6\n",
        "urls='https://www.newyorker.com/humor/borowitz-report/page/{}'\n",
        "for page in range(2,145):\n",
        "  print(page)\n",
        "  requestz=requests.get(urls.format(page)).text\n",
        "  soup=BeautifulSoup(requestz,'html.parser')\n",
        "  articles=soup.find_all('li',{'class':\"River__riverItem___3huWr\" })\n",
        "  for article in articles:\n",
        "    title=article.find('h4').get_text()\n",
        "    link='https://www.newyorker.com'+article.find('a')['href']\n",
        "    titles.append([title,link])\n",
        "    time_have_wait=time_wait+random.randint(122,255)/random.randint(30,173)- random.randint(0,2)*random.randint(0,2)\n",
        "  print('Time have to wait is %d',time_have_wait)\n",
        "  time.sleep(time_have_wait)\n",
        "df=pd.DataFrame(titles)\n",
        "df.to_csv('Borowitzreport.csv',encoding='utf-8-sig')\n",
        "files.download('Borowitzreport.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoMkRGjEXo0H"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "start='https://www.burrardstreetjournal.com/category/football-news/'\n",
        "page=requests.get(start).text\n",
        "soup=BeautifulSoup(page,'html.parser')\n",
        "articles=soup.find_all('div',{'class':\"td_module_1 td_module_wrap td-animation-stack\"})\n",
        "titles=[]\n",
        "for article in articles:\n",
        "  title=article.find('h3').get_text()\n",
        "  link=article.find('h3').find('a')['href']\n",
        "  titles.append([title,link])\n",
        "titles\n",
        "time_wait=6\n",
        "urls='https://www.burrardstreetjournal.com/category/football-news/page/{}/'\n",
        "for page in range(2,14):\n",
        "  print(page)\n",
        "  requestz=requests.get(urls.format(page)).text\n",
        "  soup=BeautifulSoup(requestz,'html.parser')\n",
        "  articles=soup.find_all('div',{'class':\"td_module_1 td_module_wrap td-animation-stack\"})\n",
        "  for article in articles:\n",
        "    title=article.find('h3').get_text()\n",
        "    link=article.find('h3').find('a')['href']\n",
        "    titles.append([title,link])\n",
        "    time_have_wait=time_wait+random.randint(122,255)/random.randint(30,173)- random.randint(0,2)*random.randint(0,15)/random.randint(8,15)\n",
        "  print('Time have to wait is %d',time_have_wait)\n",
        "  time.sleep(time_have_wait)\n",
        "startz='https://www.burrardstreetjournal.com/category/burrard-street-journal-news/trumpnews/'\n",
        "page=requests.get(startz).text\n",
        "articles=soup.find_all('div',{'class':\"td_module_1 td_module_wrap td-animation-stack\"})\n",
        "for article in articles:\n",
        "  title=article.find('h3').get_text()\n",
        "  link=article.find('h3').find('a')['href']\n",
        "  titles.append([title,link])\n",
        "urls='https://www.burrardstreetjournal.com/category/burrard-street-journal-news/trumpnews/page/{}/'\n",
        "for page in range(2,9):\n",
        "  print(page)\n",
        "  requestz=requests.get(urls.format(page)).text\n",
        "  soup=BeautifulSoup(requestz,'html.parser')\n",
        "  articles=soup.find_all('div',{'class':\"td_module_1 td_module_wrap td-animation-stack\"})\n",
        "  for article in articles:\n",
        "    title=article.find('h3').get_text()\n",
        "    link=article.find('h3').find('a')['href']\n",
        "    titles.append([title,link])\n",
        "    time_have_wait=time_wait+random.randint(122,255)/random.randint(30,173)- random.randint(0,2)*random.randint(0,15)/random.randint(8,15)\n",
        "  print('Time have to wait is %d',time_have_wait)\n",
        "  time.sleep(time_have_wait)\n",
        "titles\n",
        "df=pd.DataFrame(titles)\n",
        "df\n",
        "df.to_csv('theBSJ.csv',encoding='utf-8-sig')\n",
        "from google.colab import files\n",
        "files.download('theBSJ.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faPiWnATYTKX"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "start='http://www.thecivilian.co.nz/'\n",
        "page=requests.get(start).text\n",
        "soup=BeautifulSoup(page,'html.parser')\n",
        "articles=soup.find_all('div',{'class':\"recent-post\"})\n",
        "titles=[]\n",
        "for article in articles:\n",
        "  title=article.find('h2').get_text()\n",
        "  link=article.find('h2').find('a')['href']\n",
        "  titles.append([title,link])\n",
        "titles\n",
        "time_wait=6\n",
        "urls='http://www.thecivilian.co.nz/page/{}/'\n",
        "for page in range(2,112):\n",
        "  print(page)\n",
        "  requestz=requests.get(urls.format(page)).text\n",
        "  soup=BeautifulSoup(requestz,'html.parser')\n",
        "  articles=soup.find_all('div',{'class':\"recent-post\"})\n",
        "  for article in articles:\n",
        "    title=article.find('h2').get_text()\n",
        "    link=article.find('h2').find('a')['href']\n",
        "    titles.append([title,link])\n",
        "    time_have_wait=time_wait+random.randint(122,255)/random.randint(30,173)- random.randint(0,2)*random.randint(0,15)/random.randint(8,15)\n",
        "  print('Time have to wait is %d',time_have_wait)\n",
        "  time.sleep(time_have_wait)\n",
        "df=pd.DataFrame(titles)\n",
        "df.to_csv('thecivilian.csv',encoding='utf-8-sig')\n",
        "from google.colab import files\n",
        "files.download('thecivilian.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRngPF7WYzwe"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "start='https://clickhole.com/'\n",
        "page=requests.get(start).text\n",
        "soup=BeautifulSoup(page,'html.parser')\n",
        "articles=soup.find_all('article')\n",
        "titles=[]\n",
        "for article in articles:\n",
        "  title=article.find('h2').find('a').get_text()\n",
        "  link=article.find('h2').find('a')['href']\n",
        "  titles.append([title,link])\n",
        "titles\n",
        "time_wait=6\n",
        "time_wait1=3\n",
        "urls='https://clickhole.com/page/{}/'\n",
        "for page in range(2,1176):\n",
        "  print(page)\n",
        "  requestz=requests.get(urls.format(page)).text\n",
        "  soup=BeautifulSoup(requestz,'html.parser')\n",
        "  articles=soup.find_all('article')\n",
        "  for article in articles:\n",
        "    title=article.find('h2').find('a').get_text()\n",
        "    link=article.find('h2').find('a')['href']\n",
        "    titles.append([title,link])\n",
        "  if page %2==0:\n",
        "    time_have_wait=time_wait+random.randint(122,255)/random.randint(30,173)- random.randint(0,2)*random.randint(0,15)/random.randint(8,15)\n",
        "    print('Time have to wait is %d',time_have_wait)\n",
        "  else:\n",
        "    time_have_wait=time_wait1+random.randint(122,255)/random.randint(30,173)- random.randint(0,2)*random.randint(0,8)/random.randint(8,15)\n",
        "    print('Time have to wait is %d',time_have_wait)\n",
        "  time.sleep(time_have_wait)\n",
        "df=pd.DataFrame(titles)\n",
        "df.to_csv('theclickhole.csv',encoding='utf-8-sig')\n",
        "df\n",
        "from google.colab import files\n",
        "files.download('theclickhole.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-xs3aOlZpOQ"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "start='https://waterfordwhispersnews.com/category/politics/'\n",
        "page=requests.get(start)\n",
        "content=str(page.content,'utf-8')\n",
        "soup=BeautifulSoup(content,'html.parser')\n",
        "articles=soup.find('div',{'id':\"content\"}).find('div',{'id':\"main\"}).find_all('article')\n",
        "titles=[]\n",
        "for article in articles:\n",
        "  title=article.find('h2').find('a').get_text()\n",
        "  link=article.find('h2').find('a')['href']\n",
        "  titles.append([title,link])\n",
        "titles\n",
        "urls='https://waterfordwhispersnews.com/category/politics/page/{}/'\n",
        "time_wait=6\n",
        "time_wait1=4\n",
        "for page in range(2,185):\n",
        "  print(page)\n",
        "  requestz=requests.get(urls.format(page))\n",
        "  contentz=str(requestz.content,'utf-8')\n",
        "  soup=BeautifulSoup(contentz,'html.parser')\n",
        "  articles=soup.find('div',{'id':\"content\"}).find('div',{'id':\"main\"}).find_all('article')\n",
        "  for article in articles:\n",
        "    title=article.find('h2').find('a').get_text()\n",
        "    link=article.find('h2').find('a')['href']\n",
        "    titles.append([title,link])\n",
        "  if page %2==0:\n",
        "    time_have_wait=time_wait+random.randint(122,255)/random.randint(30,173)- random.randint(0,2)*random.randint(0,15)/random.randint(8,15)\n",
        "    print('Time have to wait is %d',time_have_wait)\n",
        "  else:\n",
        "    time_have_wait=time_wait1+random.randint(122,255)/random.randint(30,173)- random.randint(0,2)*random.randint(0,8)/random.randint(8,15)\n",
        "    print('Time have to wait is %d',time_have_wait)\n",
        "  time.sleep(time_have_wait)\n",
        "df=pd.DataFrame(titles)\n",
        "df\n",
        "df.to_csv('thewaterford.csv',encoding='utf-8-sig')\n",
        "pd.read_csv('thewaterford.csv')\n",
        "from google.colab import files\n",
        "files.download('thewaterford.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9NBGON3SEOf"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "url='https://dailybonnet.com/category/church/'\n",
        "page=requests.get(url).text\n",
        "soup=BeautifulSoup(page,'html.parser')\n",
        "articles=soup.find('div',{'id':\"main-content\"}).find_all('article')\n",
        "titles=[]\n",
        "for article in articles:\n",
        "  title=article.find('h3').get_text().strip('\\n\\r\\n\\t\\t\\t\\t\\t').strip('\\t\\t\\t\\t\\n')\n",
        "  link=article.find('h3').find('a')['href']\n",
        "  titles.append([title,link])\n",
        "titles\n",
        "time_wait=4\n",
        "urls='https://dailybonnet.com/category/church/page/{}/'\n",
        "for page in range(2,62):\n",
        "  print(page)\n",
        "  requestz=requests.get(urls.format(page)).text\n",
        "  soup=BeautifulSoup(requestz,'html.parser')\n",
        "  articles=soup.find('div',{'id':\"main-content\"}).find_all('article')\n",
        "  for article in articles:\n",
        "    title=article.find('h3').get_text().strip('\\n\\r\\n\\t\\t\\t\\t\\t').strip('\\t\\t\\t\\t\\n')\n",
        "    link=article.find('h3').find('a')['href']\n",
        "    titles.append([title,link])\n",
        "    time_have_wait=time_wait+random.randint(122,255)/random.randint(30,173)- random.randint(0,2)*random.randint(0,15)/random.randint(8,15)\n",
        "  print('Time have to wait is %d',time_have_wait)\n",
        "  time.sleep(time_have_wait)\n",
        "urlz='https://dailybonnet.com/category/food-and-drink/'\n",
        "page=requests.get(urlz).text\n",
        "soup=BeautifulSoup(page,'html.parser')\n",
        "articles=soup.find('div',{'id':\"main-content\"}).find_all('article')\n",
        "for article in articles:\n",
        "  title=article.find('h3').get_text().strip('\\n\\r\\n\\t\\t\\t\\t\\t').strip('\\t\\t\\t\\t\\n')\n",
        "  link=article.find('h3').find('a')['href']\n",
        "  titles.append([title,link])\n",
        "urlss='https://dailybonnet.com/category/food-and-drink/page/{}/'\n",
        "for page in range(2,79):\n",
        "  print(page)\n",
        "  requestz=requests.get(urlss.format(page)).text\n",
        "  soup=BeautifulSoup(requestz,'html.parser')\n",
        "  articles=soup.find('div',{'id':\"main-content\"}).find_all('article')\n",
        "  for article in articles:\n",
        "    title=article.find('h3').get_text().strip('\\n\\r\\n\\t\\t\\t\\t\\t').strip('\\t\\t\\t\\t\\n')\n",
        "    link=article.find('h3').find('a')['href']\n",
        "    titles.append([title,link])\n",
        "    time_have_wait=time_wait+random.randint(122,255)/random.randint(30,173)- random.randint(0,2)*random.randint(0,15)/random.randint(8,15)\n",
        "  print('Time have to wait is %d',time_have_wait)\n",
        "  time.sleep(time_have_wait)\n",
        "urlzz='https://dailybonnet.com/category/mennonitelife/'\n",
        "page=requests.get(urlzz).text\n",
        "soup=BeautifulSoup(page,'html.parser')\n",
        "articles=soup.find('div',{'id':\"main-content\"}).find_all('article')\n",
        "for article in articles:\n",
        "  title=article.find('h3').get_text().strip('\\n\\r\\n\\t\\t\\t\\t\\t').strip('\\t\\t\\t\\t\\n')\n",
        "  link=article.find('h3').find('a')['href']\n",
        "  titles.append([title,link])\n",
        "titles\n",
        "urlsss='https://dailybonnet.com/category/mennonitelife/page/{}/'\n",
        "for page in range(2,171):\n",
        "  print(page)\n",
        "  requestz=requests.get(urlsss.format(page)).text\n",
        "  soup=BeautifulSoup(requestz,'html.parser')\n",
        "  articles=soup.find('div',{'id':\"main-content\"}).find_all('article')\n",
        "  for article in articles:\n",
        "    title=article.find('h3').get_text().strip('\\n\\r\\n\\t\\t\\t\\t\\t').strip('\\t\\t\\t\\t\\n')\n",
        "    link=article.find('h3').find('a')['href']\n",
        "    titles.append([title,link])\n",
        "    time_have_wait=time_wait+random.randint(122,255)/random.randint(30,173)- random.randint(0,2)*random.randint(0,15)/random.randint(8,15)\n",
        "  print('Time have to wait is %d',time_have_wait)\n",
        "  time.sleep(time_have_wait)\n",
        "titles\n",
        "df=pd.DataFrame(titles)\n",
        "df.to_csv('themennonite.csv',encoding='utf-8-sig')\n",
        "from google.colab import files\n",
        "df\n",
        "files.download('themennonite.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsYBKRxcTkxw"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.support.ui import Select\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.common.exceptions import TimeoutException\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "from selenium.common.exceptions import NoAlertPresentException\n",
        "import sys\n",
        "from selenium.webdriver.firefox.options import Options\n",
        "!pip install selenium\n",
        "import time\n",
        "import random\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.support.ui import Select\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.common.exceptions import TimeoutException\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "from selenium.common.exceptions import NoAlertPresentException\n",
        "from selenium.webdriver.firefox.firefox_binary import FirefoxBinary\n",
        "binary = FirefoxBinary('path/to/installed firefox binary')\n",
        "def scroll(driver, timeout):\n",
        "    time_have_wait=timeout+random.randint(122,255)/random.randint(80,173)- random.randint(0,2)*random.randint(8,15)/random.randint(8,15)\n",
        "    scroll_pause_time = time_have_wait\n",
        "    # Get scroll height\n",
        "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "    i=0\n",
        "    current_scroll_position, new_height= 0, 1\n",
        "    while current_scroll_position <= new_height:\n",
        "      current_scroll_position += 200\n",
        "      driver.execute_script(\"window.scrollTo(0, {});\".format(current_scroll_position))\n",
        "      new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "      i+=1\n",
        "      print(i)\n",
        "      time_have_wait=timeout+random.randint(122,255)/random.randint(80,173)- random.randint(0,2)*random.randint(8,15)/random.randint(8,15)\n",
        "      print('We have to wait %d',time_have_wait)\n",
        "      time.sleep(time_have_wait)\n",
        "      time.sleep(0.3)\n",
        "        # Wait to load page\n",
        "url='https://www.cracked.com/funny-articles.html'\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "!apt-get update\n",
        "!pip install selenium\n",
        "!apt install firefox-geckodriver\n",
        "!cp /usr/lib/geckodriver /usr/bin\n",
        "!cp /usr/lib/firefox /usr/bin\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n",
        "    # implicitly_wait tells the driver to wait before throwing an exception\n",
        "driver.implicitly_wait(30)\n",
        "    # driver.get(url) opens the page\n",
        "driver.get(url)\n",
        "    # This starts the scrolling by passing the driver and a timeout\n",
        "scroll(driver, 4)\n",
        "    # Once scroll returns bs4 parsers the page_source\n",
        "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "    # Them we close the driver as soup_a is storing the page source\n",
        "driver.close()\n",
        "articles=soup.find_all('article',{'class':\"content-card\"})\n",
        "titles=[]\n",
        "for article in articles:\n",
        "  title=article.find('h2',{'class':\"title\"}).find('a').get_text()\n",
        "  link=article.find('h2',{'class':\"title\"}).find('a')['href']\n",
        "  titles.append([title,link])\n",
        "titles\n",
        "df=pd.DataFrame(titles)\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "df.to_csv('Cracked.csv',encoding='utf-8-sig')\n",
        "from google.colab import files\n",
        "df\n",
        "files.download('Cracked2.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}